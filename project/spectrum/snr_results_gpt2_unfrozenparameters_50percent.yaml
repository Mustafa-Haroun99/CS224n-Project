unfrozen_parameters:
- ^lm_head.weight$
- ^model.embed_tokens.weight$
# attn.c_attn layers
- transformer.h.9.attn.c_attn
- transformer.h.8.attn.c_attn
- transformer.h.10.attn.c_attn
- transformer.h.11.attn.c_attn
- transformer.h.7.attn.c_attn
- transformer.h.6.attn.c_attn
# attn.c_proj layers
- transformer.h.3.attn.c_proj
- transformer.h.5.attn.c_proj
- transformer.h.0.attn.c_proj
- transformer.h.6.attn.c_proj
- transformer.h.2.attn.c_proj
- transformer.h.4.attn.c_proj
# lm_head layers
# ln_1 layers
- transformer.h.0.ln_1
- transformer.h.1.ln_1
- transformer.h.2.ln_1
- transformer.h.3.ln_1
- transformer.h.4.ln_1
- transformer.h.5.ln_1
# ln_2 layers
- transformer.h.0.ln_2
- transformer.h.1.ln_2
- transformer.h.2.ln_2
- transformer.h.3.ln_2
- transformer.h.4.ln_2
- transformer.h.5.ln_2
# mlp.c_fc layers
- transformer.h.1.mlp.c_fc
- transformer.h.2.mlp.c_fc
- transformer.h.8.mlp.c_fc
- transformer.h.3.mlp.c_fc
- transformer.h.9.mlp.c_fc
- transformer.h.7.mlp.c_fc
# mlp.c_proj layers
- transformer.h.1.mlp.c_proj
- transformer.h.0.mlp.c_proj
- transformer.h.2.mlp.c_proj
- transformer.h.5.mlp.c_proj
- transformer.h.7.mlp.c_proj
- transformer.h.6.mlp.c_proj
# transformer.ln_f layers
# transformer.wpe layers
# transformer.wte layers
